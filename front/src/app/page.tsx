"use client";

import Image from "next/image";
import { useState, useRef, useEffect } from "react";

export default function Home() {
  const [transcription, setTranscription] = useState<string>("");
  const [isRecording, setIsRecording] = useState<boolean>(false);
  const [isRealTimeRecording, setIsRealTimeRecording] = useState<boolean>(false);
  const [memo, setMemo] = useState<string[]>([]);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const websocketRef = useRef<WebSocket | null>(null);
  
  // VAD ê´€ë ¨ ìƒíƒœì™€ ë³€ìˆ˜ë“¤
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const audioDataRef = useRef<Uint8Array | null>(null);
  const silenceDetectionRef = useRef<{
    triggered: boolean;
    silentFrames: number;
    voicedFrames: number;
    audioBuffer: Float32Array[];
    silenceThreshold: number;
    minSilentFrames: number;
    silentFrameThreshold: number;
  }>({
    triggered: false,
    silentFrames: 0,
    voicedFrames: 0,
    audioBuffer: [],
    silenceThreshold: 0.01, // ì†Œë¦¬ ê°ì§€ ì„ê³„ê°’ (ì¡°ì • ê°€ëŠ¥)
    minSilentFrames: 15,    // ìµœì†Œ ì¹¨ë¬µ í”„ë ˆì„ ìˆ˜ (ì•½ 0.5ì´ˆ, ì¡°ì • ê°€ëŠ¥)
    silentFrameThreshold: 0.3, // ì¹¨ë¬µìœ¼ë¡œ ê°„ì£¼í•  ë³¼ë¥¨ ì„ê³„ê°’ (ì¡°ì • ê°€ëŠ¥)
  });
  const processorRef = useRef<ScriptProcessorNode | null>(null);
  const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const framesSinceLastSendRef = useRef<number>(0);

  // ë²„íŠ¼ì„ ëˆŒëŸ¬ì„œ ë…¹ìŒ ì‹œì‘
  const startRecording = async () => {
    try {
      // ì´ë¯¸ ì§„í–‰ ì¤‘ì¸ ë…¹ìŒì´ ìˆë‹¤ë©´ ì¤‘ì§€
      if (isRealTimeRecording) {
        await stopRealTimeRecording();
      }
      
      audioChunksRef.current = [];
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const mediaRecorder = new MediaRecorder(stream);
      mediaRecorderRef.current = mediaRecorder;
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };
      
      mediaRecorder.start();
      setIsRecording(true);
    } catch (error) {
      console.error("ë…¹ìŒ ì‹œì‘ ì˜¤ë¥˜:", error);
    }
  };

  // ë²„íŠ¼ì—ì„œ ì† ë–¼ë©´ ë…¹ìŒ ì¢…ë£Œ ë° ì „ì†¡
  const stopRecording = async () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
      
      // ë…¹ìŒì´ ì™„ë£Œë˜ë©´ ì˜¤ë””ì˜¤ ì²˜ë¦¬
      mediaRecorderRef.current.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/wav' });
        await sendAudioToServer(audioBlob);
        
        // ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
        const tracks = mediaRecorderRef.current?.stream?.getTracks() || [];
        tracks.forEach(track => track.stop());
      };
    }
  };
  
  // ë…¹ìŒëœ ì˜¤ë””ì˜¤ë¥¼ ì„œë²„ë¡œ ì „ì†¡
  const sendAudioToServer = async (audioBlob: Blob) => {
    try {
      const formData = new FormData();
      formData.append("file", audioBlob, "audio.wav");
      
      const response = await fetch("http://localhost:8000/transcribe/", {
        method: "POST",
        body: formData,
      });
      
      if (!response.ok) {
        throw new Error(`ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: ${response.status}`);
      }
      
      const data = await response.json();
      setTranscription(data.transcription);
      setMemo(prev => [...prev, data.transcription]);
    } catch (error) {
      console.error("ì˜¤ë””ì˜¤ ì „ì†¡ ì˜¤ë¥˜:", error);
      setTranscription("ì„œë²„ ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë°±ì—”ë“œ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.");
    }
  };

  // ì‹œë¦¬ ìŠ¤íƒ€ì¼ ìŒì„± ê°ì§€ ë° ì²˜ë¦¬
  const processSiriStyleAudio = (audioProcessingEvent: AudioProcessingEvent) => {
    const inputBuffer = audioProcessingEvent.inputBuffer;
    const inputData = inputBuffer.getChannelData(0);
    const pcmData = new Int16Array(inputData.length);
    
    // ìŒì„± í™œì„±í™” ê°ì§€ (VAD)
    let sum = 0;
    let max = 0;
    
    // ì…ë ¥ ë°ì´í„°ì˜ ë³¼ë¥¨ ê³„ì‚° (RMS)
    for (let i = 0; i < inputData.length; i++) {
      const absValue = Math.abs(inputData[i]);
      sum += absValue * absValue;
      max = Math.max(max, absValue);
      
      // ë™ì‹œì— PCM ë°ì´í„° ë³€í™˜
      pcmData[i] = Math.max(-32768, Math.min(32767, Math.floor(inputData[i] * 32768)));
    }
    
    const rms = Math.sqrt(sum / inputData.length);
    const isSilent = rms < silenceDetectionRef.current.silentFrameThreshold;
    
    // ìŒì„± ë²„í¼ì— í˜„ì¬ í”„ë ˆì„ ì¶”ê°€
    silenceDetectionRef.current.audioBuffer.push(new Float32Array(inputData));
    
    // ìŒì„± ìƒíƒœ ì—…ë°ì´íŠ¸
    if (!silenceDetectionRef.current.triggered) {
      if (!isSilent) {
        // ìŒì„± ê°ì§€ ì‹œì‘
        console.log("ğŸ™ï¸ ìŒì„± ê°ì§€ë¨ - ë…¹ìŒ ì‹œì‘");
        silenceDetectionRef.current.triggered = true;
        silenceDetectionRef.current.voicedFrames = 1;
        silenceDetectionRef.current.silentFrames = 0;
      }
    } else {
      if (!isSilent) {
        // ìŒì„± ê³„ì† ê°ì§€
        silenceDetectionRef.current.voicedFrames++;
        silenceDetectionRef.current.silentFrames = 0;
      } else {
        // ì¹¨ë¬µ ê°ì§€
        silenceDetectionRef.current.silentFrames++;
        
        // ì¼ì • ì‹œê°„ ì´ìƒ ì¹¨ë¬µì´ ì§€ì†ë˜ë©´ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ì¢…ë£Œ
        if (silenceDetectionRef.current.silentFrames > silenceDetectionRef.current.minSilentFrames) {
          console.log(`âœ… ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ì¢…ë£Œ (ìŒì„± í”„ë ˆì„: ${silenceDetectionRef.current.voicedFrames})`);
          
          if (silenceDetectionRef.current.voicedFrames > 5) { // ìµœì†Œí•œì˜ ìœ ì˜ë¯¸í•œ ìŒì„± í”„ë ˆì„ ìˆ˜
            // ì˜¤ë””ì˜¤ ë°ì´í„° ì¤€ë¹„ ë° ì „ì†¡
            const allAudioData = concatenateAudioBuffers(silenceDetectionRef.current.audioBuffer);
            
            // WAV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            const wavBuffer = createWavFromAudioBuffer(allAudioData, 16000);
            
            // ì›¹ì†Œì¼“ìœ¼ë¡œ ì „ì†¡
            if (websocketRef.current?.readyState === WebSocket.OPEN) {
              websocketRef.current.send(wavBuffer);
            }
          }
          
          // ìƒíƒœ ì´ˆê¸°í™”
          silenceDetectionRef.current.triggered = false;
          silenceDetectionRef.current.voicedFrames = 0;
          silenceDetectionRef.current.silentFrames = 0;
          silenceDetectionRef.current.audioBuffer = [];
        }
      }
    }
    
    // ë²„í¼ê°€ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šë„ë¡ ì œí•œ
    if (silenceDetectionRef.current.audioBuffer.length > 300) { // ì•½ 10ì´ˆ ë¶„ëŸ‰
      silenceDetectionRef.current.audioBuffer = [];
      silenceDetectionRef.current.triggered = false;
      silenceDetectionRef.current.voicedFrames = 0;
      silenceDetectionRef.current.silentFrames = 0;
      console.log("âš ï¸ ì˜¤ë””ì˜¤ ë²„í¼ê°€ ë„ˆë¬´ ì»¤ì„œ ì´ˆê¸°í™”ë¨");
    }
  };

  // Float32Array ë°°ì—´ì„ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ í•©ì¹˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
  const concatenateAudioBuffers = (buffers: Float32Array[]): Float32Array => {
    let totalLength = 0;
    for (const buffer of buffers) {
      totalLength += buffer.length;
    }
    
    const result = new Float32Array(totalLength);
    let offset = 0;
    
    for (const buffer of buffers) {
      result.set(buffer, offset);
      offset += buffer.length;
    }
    
    return result;
  };

  // Float32Array ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ WAV í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
  const createWavFromAudioBuffer = (audioBuffer: Float32Array, sampleRate: number): ArrayBuffer => {
    // PCM ë°ì´í„°ë¡œ ë³€í™˜
    const pcmData = new Int16Array(audioBuffer.length);
    for (let i = 0; i < audioBuffer.length; i++) {
      pcmData[i] = Math.max(-32768, Math.min(32767, Math.floor(audioBuffer[i] * 32768)));
    }
    
    // WAV í—¤ë” ìƒì„±
    const wavBuffer = new ArrayBuffer(44 + pcmData.byteLength);
    const view = new DataView(wavBuffer);
    
    // RIFF ì²­í¬ í—¤ë”
    writeString(view, 0, 'RIFF');
    view.setUint32(4, 36 + pcmData.byteLength, true);
    writeString(view, 8, 'WAVE');
    
    // fmt í•˜ìœ„ ì²­í¬
    writeString(view, 12, 'fmt ');
    view.setUint32(16, 16, true); // fmt ì²­í¬ í¬ê¸°
    view.setUint16(20, 1, true); // ì˜¤ë””ì˜¤ í¬ë§· (1 = PCM)
    view.setUint16(22, 1, true); // ì±„ë„ ìˆ˜
    view.setUint32(24, sampleRate, true); // ìƒ˜í”Œë ˆì´íŠ¸
    view.setUint32(28, sampleRate * 2, true); // ë°”ì´íŠ¸ ë ˆì´íŠ¸
    view.setUint16(32, 2, true); // ë¸”ë¡ ì–¼ë¼ì¸
    view.setUint16(34, 16, true); // ë¹„íŠ¸ ëìŠ¤
    
    // ë°ì´í„° í•˜ìœ„ ì²­í¬
    writeString(view, 36, 'data');
    view.setUint32(40, pcmData.byteLength, true);
    
    // ì˜¤ë””ì˜¤ ë°ì´í„° ì“°ê¸°
    const pcmBytes = new Uint8Array(wavBuffer, 44);
    const pcmByteView = new Uint8Array(pcmData.buffer);
    pcmBytes.set(pcmByteView);
    
    return wavBuffer;
  };
  
  // DataViewì— ë¬¸ìì—´ ì“°ëŠ” í—¬í¼ í•¨ìˆ˜
  const writeString = (view: DataView, offset: number, string: string): void => {
    for (let i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i));
    }
  };

  // ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œì‘
  const startRealTimeRecording = async () => {
    try {
      // ì´ë¯¸ ì§„í–‰ ì¤‘ì¸ ì¼ë°˜ ë…¹ìŒì´ ìˆë‹¤ë©´ ì¤‘ì§€
      if (isRecording) {
        await stopRecording();
      }
      
      // ì´ë¯¸ ì‹¤ì‹œê°„ ë…¹ìŒì´ ì§„í–‰ ì¤‘ì´ë¼ë©´ ì¤‘ì§€ í›„ ì¬ì‹œì‘
      if (isRealTimeRecording) {
        await stopRealTimeRecording();
        // ë¦¬ì†ŒìŠ¤ ì •ë¦¬ë¥¼ ìœ„í•´ ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€
        await new Promise(resolve => setTimeout(resolve, 300));
      }
      
      // WebSocket ì—°ê²°
      websocketRef.current = new WebSocket("ws://localhost:8000/ws/transcribe/");
      
      // ì›¹ì†Œì¼“ ì—°ê²° íƒ€ì„ì•„ì›ƒ ì„¤ì •
      const wsConnectionTimeout = setTimeout(() => {
        if (websocketRef.current?.readyState !== WebSocket.OPEN) {
          console.error("WebSocket ì—°ê²° íƒ€ì„ì•„ì›ƒ");
          setTranscription("ì„œë²„ ì—°ê²° ì˜¤ë¥˜: ì›¹ì†Œì¼“ ì—°ê²° ì‹œê°„ ì´ˆê³¼. ë°±ì—”ë“œ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.");
          if (websocketRef.current) {
            websocketRef.current.close();
            websocketRef.current = null;
          }
          setIsRealTimeRecording(false);
        }
      }, 5000); // 5ì´ˆ íƒ€ì„ì•„ì›ƒ
      
      websocketRef.current.onopen = () => {
        console.log("WebSocket ì—°ê²°ë¨");
        clearTimeout(wsConnectionTimeout);
        setIsRealTimeRecording(true);
      };
      
      websocketRef.current.onmessage = (event) => {
        const text = event.data;
        console.log("ì„œë²„ë¡œë¶€í„° ì‘ë‹µ ë°›ìŒ:", text);
        if (text && text.trim() !== "") {
          setTranscription(text);
          setMemo(prev => [...prev, text]);
        }
      };
      
      websocketRef.current.onerror = (error) => {
        console.error("WebSocket ì˜¤ë¥˜:", error);
        setTranscription("ì›¹ì†Œì¼“ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë°±ì—”ë“œ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.");
      };
      
      websocketRef.current.onclose = (event) => {
        console.log(`WebSocket ì—°ê²° ì¢…ë£Œ (ì½”ë“œ: ${event.code}, ì´ìœ : ${event.reason})`);
        clearTimeout(wsConnectionTimeout);
        setIsRealTimeRecording(false);
      };
      
      // ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì„¤ì •
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      
      streamRef.current = stream;
      
      // ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
      try {
        const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({
          sampleRate: 16000,
          latencyHint: 'interactive'
        });
        
        audioContextRef.current = audioContext;
        
        // ì˜¤ë””ì˜¤ ì†ŒìŠ¤ ë° ë¶„ì„ê¸° ìƒì„±
        const source = audioContext.createMediaStreamSource(stream);
        sourceRef.current = source;
        
        // ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë…¸ë“œ ìƒì„±
        const processor = audioContext.createScriptProcessor(4096, 1, 1);
        processorRef.current = processor;
        
        // VAD ë¶„ì„ì„ ìœ„í•œ ë¶„ì„ê¸° ë…¸ë“œ ìƒì„±
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048;
        analyserRef.current = analyser;
        
        const audioData = new Uint8Array(analyser.frequencyBinCount);
        audioDataRef.current = audioData;
        
        // ì—°ê²°: ì†ŒìŠ¤ -> ë¶„ì„ê¸° -> í”„ë¡œì„¸ì„œ -> ì¶œë ¥
        source.connect(analyser);
        source.connect(processor);
        processor.connect(audioContext.destination);
        
        // ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •
        processor.onaudioprocess = processSiriStyleAudio;
        
        // ì´ˆê¸°í™”
        silenceDetectionRef.current = {
          triggered: false,
          silentFrames: 0,
          voicedFrames: 0,
          audioBuffer: [],
          silenceThreshold: 0.01,
          minSilentFrames: 15,
          // ì¹¨ë¬µ ì„ê³„ê°’ ì¡°ì • - ë” ë‚®ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ìŒì„± ê°ì§€ ë¯¼ê°ë„ í–¥ìƒ
          silentFrameThreshold: 0.015, 
        };
        
        setIsRealTimeRecording(true);
      } catch (audioError) {
        console.error("ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì„¤ì • ì˜¤ë¥˜:", audioError);
        // ì˜¤ë¥˜ ë°œìƒ ì‹œ ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
        if (stream) {
          stream.getTracks().forEach(track => track.stop());
        }
        if (websocketRef.current) {
          websocketRef.current.close();
          websocketRef.current = null;
        }
        throw audioError;
      }
    } catch (error) {
      console.error("ì‹¤ì‹œê°„ ë…¹ìŒ ì‹œì‘ ì˜¤ë¥˜:", error);
      setTranscription("ì„œë²„ ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë°±ì—”ë“œ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.");
      setIsRealTimeRecording(false);
    }
  };

  // ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì¢…ë£Œ
  const stopRealTimeRecording = () => {
    return new Promise<void>((resolve) => {
      if (isRealTimeRecording) {
        console.log("ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì •ë¦¬ ì¤‘...");
        
        // ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ì§€
        if (processorRef.current) {
          try {
            processorRef.current.disconnect();
          } catch (e) {
            console.warn("í”„ë¡œì„¸ì„œ ì—°ê²° í•´ì œ ì˜¤ë¥˜:", e);
          }
          processorRef.current = null;
        }
        
        if (sourceRef.current) {
          try {
            sourceRef.current.disconnect();
          } catch (e) {
            console.warn("ì†ŒìŠ¤ ì—°ê²° í•´ì œ ì˜¤ë¥˜:", e);
          }
          sourceRef.current = null;
        }
        
        if (audioContextRef.current) {
          try {
            audioContextRef.current.close();
          } catch (e) {
            console.warn("ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ ì˜¤ë¥˜:", e);
          }
          audioContextRef.current = null;
        }
        
        // ë¶„ì„ê¸° ì •ë¦¬
        analyserRef.current = null;
        audioDataRef.current = null;
        
        // ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
        if (streamRef.current) {
          streamRef.current.getTracks().forEach(track => {
            try {
              track.stop();
            } catch (e) {
              console.warn("íŠ¸ë™ ì¢…ë£Œ ì˜¤ë¥˜:", e);
            }
          });
          streamRef.current = null;
        }
        
        // WebSocket ì—°ê²° ì¢…ë£Œ
        if (websocketRef.current) {
          try {
            // WebSocketì´ ì•„ì§ ì—°ê²° ì¤‘ì´ë¼ë©´ ì •ìƒ ì¢…ë£Œ
            if (websocketRef.current.readyState === WebSocket.OPEN || 
                websocketRef.current.readyState === WebSocket.CONNECTING) {
              websocketRef.current.close(1000, "ì‚¬ìš©ìê°€ ë…¹ìŒ ì¢…ë£Œ");
            }
          } catch (e) {
            console.warn("ì›¹ì†Œì¼“ ì¢…ë£Œ ì˜¤ë¥˜:", e);
          }
          websocketRef.current = null;
        }
        
        // VAD ìƒíƒœ ì´ˆê¸°í™”
        silenceDetectionRef.current = {
          triggered: false,
          silentFrames: 0,
          voicedFrames: 0,
          audioBuffer: [],
          silenceThreshold: 0.01,
          minSilentFrames: 15,
          silentFrameThreshold: 0.3,
        };
        
        setIsRealTimeRecording(false);
        
        // ì•½ê°„ì˜ ì§€ì—° í›„ì— ì •ë¦¬ ì™„ë£Œ ì•Œë¦¼
        setTimeout(() => {
          console.log("ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ");
          resolve();
        }, 100);
      } else {
        resolve();
      }
    });
  };

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
  useEffect(() => {
    return () => {
      console.log("ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ - ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘");
      
      // ì¼ë°˜ ë…¹ìŒ ì •ë¦¬
      if (mediaRecorderRef.current && mediaRecorderRef.current.stream) {
        try {
          const tracks = mediaRecorderRef.current.stream.getTracks();
          tracks.forEach(track => track.stop());
        } catch (e) {
          console.warn("MediaRecorder ìŠ¤íŠ¸ë¦¼ ì •ë¦¬ ì˜¤ë¥˜:", e);
        }
        mediaRecorderRef.current = null;
      }
      
      // ì‹¤ì‹œê°„ ë…¹ìŒ ì •ë¦¬
      if (processorRef.current) {
        try {
          processorRef.current.disconnect();
        } catch (e) {
          console.warn("í”„ë¡œì„¸ì„œ ì—°ê²° í•´ì œ ì˜¤ë¥˜:", e);
        }
        processorRef.current = null;
      }
      
      if (sourceRef.current) {
        try {
          sourceRef.current.disconnect();
        } catch (e) {
          console.warn("ì†ŒìŠ¤ ì—°ê²° í•´ì œ ì˜¤ë¥˜:", e);
        }
        sourceRef.current = null;
      }
      
      if (audioContextRef.current) {
        try {
          audioContextRef.current.close();
        } catch (e) {
          console.warn("ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ ì˜¤ë¥˜:", e);
        }
        audioContextRef.current = null;
      }
      
      if (streamRef.current) {
        try {
          streamRef.current.getTracks().forEach(track => track.stop());
        } catch (e) {
          console.warn("ìŠ¤íŠ¸ë¦¼ íŠ¸ë™ ì •ë¦¬ ì˜¤ë¥˜:", e);
        }
        streamRef.current = null;
      }
      
      // WebSocket ì—°ê²° ì¢…ë£Œ
      if (websocketRef.current) {
        try {
          websocketRef.current.close();
        } catch (e) {
          console.warn("ì›¹ì†Œì¼“ ì¢…ë£Œ ì˜¤ë¥˜:", e);
        }
        websocketRef.current = null;
      }
      
      console.log("ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ");
    };
  }, []);

  return (
    <div className="grid grid-rows-[20px_1fr_20px] items-center justify-items-center min-h-screen p-8 pb-20 gap-16 sm:p-20 font-[family-name:var(--font-geist-sans)]">
      <main className="flex flex-col gap-[32px] row-start-2 items-center sm:items-start w-full max-w-3xl">
        <Image
          className="dark:invert"
          src="/next.svg"
          alt="Next.js logo"
          width={180}
          height={38}
          priority
        />
        
        {/* ìŒì„± ë©”ëª¨ ì„¹ì…˜ */}
        <div className="w-full p-6 bg-gray-50 dark:bg-gray-800 rounded-lg shadow-md">
          <h2 className="text-2xl font-bold mb-4">ìŒì„± ë©”ëª¨</h2>
          
          {/* ìŒì„± ë©”ëª¨ ë²„íŠ¼ */}
          <div className="flex flex-col sm:flex-row gap-4 mb-6">
            <button
              className={`rounded-full px-6 py-3 font-medium ${
                isRecording 
                  ? "bg-red-500 text-white" 
                  : "bg-black text-white dark:bg-white dark:text-black"
              }`}
              onMouseDown={startRecording}
              onMouseUp={stopRecording}
              onTouchStart={startRecording}
              onTouchEnd={stopRecording}
            >
              {isRecording ? "ë…¹ìŒ ì¤‘..." : "ë²„íŠ¼ì„ ëˆ„ë¥´ê³  ë§í•˜ê¸°"}
            </button>
            
            <button
              className={`rounded-full px-6 py-3 font-medium ${
                isRealTimeRecording 
                  ? "bg-red-500 text-white" 
                  : "bg-green-600 text-white"
              }`}
              onClick={isRealTimeRecording ? stopRealTimeRecording : startRealTimeRecording}
            >
              {isRealTimeRecording ? "ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì¤‘ì§€" : "ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œì‘"}
            </button>
          </div>
          
          {/* í˜„ì¬ ì¸ì‹ëœ í…ìŠ¤íŠ¸ */}
          {transcription && (
            <div className="mb-4 p-3 bg-white dark:bg-gray-700 rounded border border-gray-200 dark:border-gray-600">
              <p className="font-medium">í˜„ì¬ ì¸ì‹ëœ í…ìŠ¤íŠ¸:</p>
              <p className="mt-2">{transcription}</p>
            </div>
          )}
          
          {/* ë©”ëª¨ ëª©ë¡ */}
          <div className="mt-6">
            <h3 className="text-lg font-semibold mb-2">ì €ì¥ëœ ë©”ëª¨</h3>
            {memo.length > 0 ? (
              <ul className="space-y-2">
                {memo.map((text, index) => (
                  <li key={index} className="p-3 bg-white dark:bg-gray-700 rounded border border-gray-200 dark:border-gray-600">
                    {text}
                  </li>
                ))}
              </ul>
            ) : (
              <p className="text-gray-500 dark:text-gray-400">ì €ì¥ëœ ë©”ëª¨ê°€ ì—†ìŠµë‹ˆë‹¤.</p>
            )}
          </div>
        </div>
      </main>
      <footer className="row-start-3 flex gap-[24px] flex-wrap items-center justify-center">
        <a
          className="flex items-center gap-2 hover:underline hover:underline-offset-4"
          href="https://nextjs.org/learn?utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app"
          target="_blank"
          rel="noopener noreferrer"
        >
          <Image
            aria-hidden
            src="/file.svg"
            alt="File icon"
            width={16}
            height={16}
          />
          Learn
        </a>
        <a
          className="flex items-center gap-2 hover:underline hover:underline-offset-4"
          href="https://vercel.com/templates?framework=next.js&utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app"
          target="_blank"
          rel="noopener noreferrer"
        >
          <Image
            aria-hidden
            src="/window.svg"
            alt="Window icon"
            width={16}
            height={16}
          />
          Examples
        </a>
        <a
          className="flex items-center gap-2 hover:underline hover:underline-offset-4"
          href="https://nextjs.org?utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app"
          target="_blank"
          rel="noopener noreferrer"
        >
          <Image
            aria-hidden
            src="/globe.svg"
            alt="Globe icon"
            width={16}
            height={16}
          />
          Go to nextjs.org â†’
        </a>
      </footer>
    </div>
  );
}
